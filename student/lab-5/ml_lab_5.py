# -*- coding: utf-8 -*-
"""ml-lab-5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GH_YyvCRYFWhH-dUR-9c-f7jqlIiRfCc

# **Задание:**

1. Выберите набор данных (датасет) для решения задачи классификации или регресии.
2. В случае необходимости проведите удаление или заполнение пропусков и кодирование категориальных признаков.
3. С использованием метода train_test_split разделите выборку на обучающую и тестовую.
4. Обучите 1) одну из линейных моделей, 2) SVM и 3) дерево решений. Оцените качество моделей с помощью трех подходящих для задачи метрик. Сравните качество полученных моделей.
5. Произведите для каждой модели подбор одного гиперпараметра с использованием GridSearchCV и кросс-валидации.
6. Повторите пункт 4 для найденных оптимальных значений гиперпараметров. Сравните качество полученных моделей с качеством моделей, полученных в пункте 4.

Датасет: [wine](https://www.kaggle.com/brynja/wineuci/downloads/wineuci.zip/1)
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import learning_curve, validation_curve
from sklearn.model_selection import KFold, RepeatedKFold, LeaveOneOut, LeavePOut, ShuffleSplit, StratifiedKFold
from sklearn.model_selection import cross_val_score, cross_validate
from sklearn.metrics import roc_curve,confusion_matrix, roc_auc_score, accuracy_score, balanced_accuracy_score

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LinearRegression

import warnings

warnings.filterwarnings('ignore')
plt.style.use('ggplot')

# Считывание данных
data = pd.read_csv('Wine.csv', sep=";")
data.head()

# Типы данных
data.dtypes

# Проверка на пустые значения
for col in data.columns:
    print('{} - {}'.format(col, data[data[col].isnull()].shape[0]))

# Размерность данных
data.shape

CLASS = 'Class'
RANDOM_STATE = 17
TEST_SIZE = 0.3

X = data.drop(CLASS, axis=1).values
Y = data[CLASS].values

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=Y)
print('X_train: {}'.format(X_train.shape))
print('X_test: {}'.format(X_test.shape))

"""# **Обучите 1) одну из линейных моделей, 2) SVM и 3) дерево решений. Оцените качество моделей с помощью трех подходящих для задачи метрик. Сравните качество полученных моделей.**

SVM
"""

# SVM
clf = SVC(gamma='auto')
clf.fit(X_train, Y_train)
clf.score(X_test, Y_test)

Y_pred = clf.predict(X_test)
print(classification_report(Y_test, Y_pred))

"""DTREE"""

# Decision tree
tree = DecisionTreeClassifier(random_state=0)
tree.fit(X_train, Y_train)
tree.score(X_test, Y_test)

Y_pred = tree.predict(X_test)
print(classification_report(Y_test, Y_pred))

"""LINEAR REGRESSION"""

lin = LinearRegression()
lin.fit(X_train, Y_train)
lin.score(X_test, Y_test)

"""# **Произведите для каждой модели подбор одного гиперпараметра с использованием GridSearchCV и кросс-валидации.**

SVM

Основная идея метода — перевод исходных векторов в пространство более высокой размерности и поиск разделяющей гиперплоскости с максимальным зазором в этом пространстве. Две параллельных гиперплоскости строятся по обеим сторонам гиперплоскости, разделяющей классы. Разделяющей гиперплоскостью будет гиперплоскость, максимизирующая расстояние до двух параллельных гиперплоскостей. Алгоритм работает в предположении, что чем больше разница или расстояние между этими параллельными гиперплоскостями, тем меньше будет средняя ошибка классификатора.
"""

CROSS_VALIDATOR_GENERATOR = 5
PARAMETER_TAG = 'C' 
PARAMETER_MAX_VALUE = 3

param_grid = {PARAMETER_TAG : np.arange(0.01, PARAMETER_MAX_VALUE, 0.01)}
clf = SVC(gamma='auto')

clf_cv = GridSearchCV(clf, param_grid, cv = CROSS_VALIDATOR_GENERATOR)
clf_cv.fit(X_train,Y_train)
clf_cv.best_score_

clf_cv.best_params_

clf = SVC(gamma='auto', C = clf_cv.best_params_[PARAMETER_TAG])
clf.fit(X_train, Y_train)
clf.score(X_test, Y_test)

Y_pred = clf.predict(X_test)
print(classification_report(Y_test, Y_pred))

"""DTREE

Структура дерева представляет собой «листья» и «ветки». На рёбрах («ветках») дерева решения записаны атрибуты, от которых зависит целевая функция, в «листьях» записаны значения целевой функции, а в остальных узлах — атрибуты, по которым различаются случаи. Чтобы классифицировать новый случай, надо спуститься по дереву до листа и выдать соответствующее значение

![alt text](https://upload.wikimedia.org/wikipedia/ru/thumb/c/c4/CART_tree_titanic_survivors_%28RU%29.svg/1024px-CART_tree_titanic_survivors_%28RU%29.svg.png)
"""

PARAMETER_TAG = 'min_impurity_decrease'

param_grid = {PARAMETER_TAG : np.arange(0.01, PARAMETER_MAX_VALUE, 0.01)}
tree = DecisionTreeClassifier(random_state=0)


tree_cv = GridSearchCV(tree, param_grid, cv = CROSS_VALIDATOR_GENERATOR)
tree_cv.fit(X_train,Y_train)
tree_cv.best_score_

tree_cv.best_params_

# Decision tree
tree = DecisionTreeClassifier(random_state=0, min_impurity_decrease = tree_cv.best_params_[PARAMETER_TAG])
tree.fit(X_train, Y_train)
tree.score(X_test, Y_test)

Y_pred = tree.predict(X_test)
print(classification_report(Y_test, Y_pred))

"""LINEAR REGRESSION

егрессио́нный анализ — статистический метод исследования влияния одной или нескольких независимых переменных {\displaystyle X_{1},X_{2},...,X_{p}} X_{1},X_{2},...,X_{p} на зависимую переменную {\displaystyle Y} Y
"""

PARAMETER_TAG = 'n_jobs'

param_grid = {PARAMETER_TAG : np.arange(1, 100)}
lin = LinearRegression()


lin_cv = GridSearchCV(lin, param_grid, cv = CROSS_VALIDATOR_GENERATOR)
lin_cv.fit(X_train, Y_train)
lin_cv.best_score_

lin_cv.best_params_

lin = LinearRegression(n_jobs = lin_cv.best_params_[PARAMETER_TAG])
lin.fit(X_train, Y_train)
lin.score(X_test, Y_test)